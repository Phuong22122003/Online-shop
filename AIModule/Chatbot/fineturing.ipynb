{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T14:25:21.993883Z",
     "iopub.status.busy": "2024-11-23T14:25:21.993541Z",
     "iopub.status.idle": "2024-11-23T14:25:27.219257Z",
     "shell.execute_reply": "2024-11-23T14:25:27.218155Z",
     "shell.execute_reply.started": "2024-11-23T14:25:21.993841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1312afb53a4397932f93b3be37c278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import random\n",
    "data_frame = pd.read_csv('/kaggle/input/custom-data-for-fashion-chatbot/dataset.csv')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('NlpHUST/gpt2-vietnamese')\n",
    "tokenizer.pad_token = '<pad>'\n",
    "tokenizer.pad_token_id = 1\n",
    "\n",
    "questions = data_frame.question\n",
    "answers = data_frame.answer\n",
    "\n",
    "texts = [f\"Hỏi: {q} Trả lời: {a} |\" for q, a in zip(questions, answers)]\n",
    "random.shuffle(texts)\n",
    "dataset = Dataset.from_dict({'text': texts})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'],return_tensors=\"pt\",padding=\"max_length\", truncation=True, max_length=768)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(len(questions), len(answers))\n",
    "print(texts[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(torch.tensor(500)))\n",
    "# print(dataset['input_ids'][80])\n",
    "print(tokenizer.encode('[SEP]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T14:47:15.289698Z",
     "iopub.status.busy": "2024-11-23T14:47:15.288930Z",
     "iopub.status.idle": "2024-11-23T14:47:15.776991Z",
     "shell.execute_reply": "2024-11-23T14:47:15.776028Z",
     "shell.execute_reply.started": "2024-11-23T14:47:15.289657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('NlpHUST/gpt2-vietnamese')\n",
    "# model = GPT2LMHeadModel.from_pretrained('minhtoan/vietnamese-gpt2-finetune')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Cấu hình optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-11-23T14:44:50.941513Z",
     "iopub.status.busy": "2024-11-23T14:44:50.941134Z",
     "iopub.status.idle": "2024-11-23T14:44:50.946907Z",
     "shell.execute_reply": "2024-11-23T14:44:50.945952Z",
     "shell.execute_reply.started": "2024-11-23T14:44:50.941480Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:25:53.480989Z",
     "iopub.status.busy": "2024-11-23T15:25:53.480157Z",
     "iopub.status.idle": "2024-11-23T15:28:19.504772Z",
     "shell.execute_reply": "2024-11-23T15:28:19.503851Z",
     "shell.execute_reply.started": "2024-11-23T15:25:53.480952Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.42006354850448974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # Để hiển thị tiến trình\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\", leave=False):\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)  # Lấy attention_mask\n",
    "        labels = inputs.clone()\n",
    "        # labels[:, :-1] = inputs[:, 1:].clone()  # Dịch các token đi 1 bước\n",
    "        # labels[:, -1] = tokenizer.pad_token_id  # Gán token pad cho token cuối cùng, vì không có token tiếp theo\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels,attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        # Cập nhật tham số\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T15:46:30.804970Z",
     "iopub.status.busy": "2024-11-23T15:46:30.804659Z",
     "iopub.status.idle": "2024-11-23T15:46:32.110133Z",
     "shell.execute_reply": "2024-11-23T15:46:32.109207Z",
     "shell.execute_reply.started": "2024-11-23T15:46:30.804944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1:\n",
      "Hỏi: Chọn đồ gì đi ăn đám cưới đây Trả lời: Hãy chọn trang phục đơn giản, nhẹ nhàng như áo thun, quần jeans, giày bệt hoặc giày cao gót để tạo cảm giác thoải mái khi đi dự tiệc. Màu sắc phù hợp là những gam màu trung tính như đen, xám hoặc trắng.\n"
     ]
    }
   ],
   "source": [
    "text = 'Chọn đồ gì đi ăn đám cưới đây';\n",
    "input_text = f\"Hỏi: {text} Trả lời:\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "model.eval()\n",
    "outputs = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    max_length=128,\n",
    "    min_length=10,\n",
    "    top_k=40,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "\n",
    "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "for i, text in enumerate(generated_texts):\n",
    "    text = text.split('|')[0]\n",
    "    text = text.split('.')\n",
    "    text = text[:2]\n",
    "    text = [t for t in text if 'http' not in t]\n",
    "    text = '.'.join(text)\n",
    "    print(f\"Generated Text {i+1}:\\n{text}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lưu mô hình sau khi huấn luyện\n",
    "model.save_pretrained('./trained_gpt2_vietnamese')\n",
    "tokenizer.save_pretrained('./trained_gpt2_vietnamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained('/kaggle/input/gpt-fineturing')\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6147566,
     "sourceId": 9989163,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6147991,
     "sourceId": 9992014,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
